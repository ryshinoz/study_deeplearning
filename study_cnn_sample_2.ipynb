{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/data.2017-02-09T14:15:57.113476\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 112\n",
    "INPUT_SIZE = 96\n",
    "DST_INPUT_SIZE = 56\n",
    "NUM_CLASS = 5\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 5000\n",
    "BATCH_SIZE = 100\n",
    "LOGDIR='/tmp/data.%s' % datetime.now().isoformat()\n",
    "\n",
    "print(LOGDIR)\n",
    "\n",
    "def load(csv_files):\n",
    "    file_queue = tf.train.string_input_producer(csv_files)\n",
    "    reader = tf.TextLineReader()\n",
    "    key, value = reader.read(file_queue)\n",
    "    filename, label = tf.decode_csv(value, [['path'], [1]])\n",
    "\n",
    "    label = tf.cast(label, tf.int64)\n",
    "    label = tf.one_hot(label, depth = NUM_CLASS, on_value = 1.0, off_value = 0.0, axis = -1)\n",
    "    \n",
    "    jpeg = tf.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(jpeg, channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image.set_shape([IMAGE_SIZE, IMAGE_SIZE, 3])\n",
    "\n",
    "    cropsize = random.randint(INPUT_SIZE, INPUT_SIZE + (IMAGE_SIZE - INPUT_SIZE) / 2)\n",
    "    framesize = INPUT_SIZE + (cropsize - INPUT_SIZE) * 2\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, framesize, framesize)\n",
    "    image = tf.random_crop(image, [cropsize, cropsize, 3])\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.8)\n",
    "    image = tf.image.random_contrast(image, lower=0.8, upper=1.0)\n",
    "    image = tf.image.random_hue(image, max_delta=0.04)\n",
    "    image = tf.image.random_saturation(image, lower=0.6, upper=1.4)\n",
    "\n",
    "    image = tf.image.resize_images(image, (DST_INPUT_SIZE, DST_INPUT_SIZE))\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "\n",
    "    min_fraction_of_examples_in_queue = 0.4\n",
    "    min_queue_examples = int(NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN * min_fraction_of_examples_in_queue)\n",
    "\n",
    "    images, label_batch, filename = tf.train.shuffle_batch(\n",
    "       [image, label, filename],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        capacity=min_queue_examples + BATCH_SIZE * 3,\n",
    "        min_after_dequeue=min_queue_examples,\n",
    "        num_threads=8\n",
    "    )\n",
    "\n",
    "    tf.summary.image('image', images, max_outputs=100)\n",
    "    labels = tf.reshape(label_batch, [BATCH_SIZE, NUM_CLASS])\n",
    "\n",
    "    \n",
    "    return (images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def inference(images, keep_prob):\n",
    "    \n",
    "    print(images)\n",
    "    x_image = tf.reshape(images, [-1, DST_INPUT_SIZE, DST_INPUT_SIZE, 3])\n",
    "\n",
    "    with tf.name_scope('filter1'):\n",
    "        \n",
    "        num_filters1 = 32\n",
    "\n",
    "        W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 3, num_filters1], stddev = 0.1))\n",
    "        b_conv1 = tf.Variable(tf.constant(0.1, shape=[num_filters1]))\n",
    "        h_conv1 = tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME')\n",
    "    \n",
    "    with tf.name_scope('pool1'):\n",
    "        h_conv1_cutoff = tf.nn.relu(h_conv1 + b_conv1)\n",
    "        h_pool1 = tf.nn.max_pool(h_conv1_cutoff, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    with tf.name_scope('filter2'):\n",
    "        num_filters2 = 64\n",
    "\n",
    "        W_conv2 = tf.Variable( tf.truncated_normal([5,5, num_filters1, num_filters2],stddev=0.1)) \n",
    "        b_conv2 = tf.Variable(tf.constant(0.1, shape=[num_filters2]))\n",
    "        h_conv2 = tf.nn.conv2d(h_pool1, W_conv2, strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    with tf.name_scope('pool2'):\n",
    "        h_conv2_cutoff = tf.nn.relu(h_conv2 + b_conv2)\n",
    "        h_pool2 = tf.nn.max_pool(h_conv2_cutoff, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "    with tf.name_scope('filter3'):\n",
    "        num_filters3 = 128\n",
    "        \n",
    "        W_conv3 = tf.Variable(tf.truncated_normal([5, 5, num_filters2, num_filters3], stddev=0.1))\n",
    "        b_conv3 = tf.Variable(tf.constant(0.1, shape=[num_filters3]))\n",
    "        h_conv3 = tf.nn.conv2d(h_pool2, W_conv3, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        \n",
    "    with tf.name_scope('pool3'):\n",
    "        h_conv3_cutoff = tf.nn.relu(h_conv3 + b_conv3)\n",
    "        h_pool3 = tf.nn.max_pool(h_conv3_cutoff, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "    with tf.name_scope('fc1'):\n",
    "        w = int(DST_INPUT_SIZE / 8)\n",
    "        h_pool3_flat = tf.reshape(h_pool3, [-1, w*w*num_filters3])\n",
    "\n",
    "        num_units1 = w*w*num_filters3\n",
    "        num_units2 = 1024\n",
    "\n",
    "        w3 = tf.Variable(tf.truncated_normal([num_units1, num_units2]))\n",
    "        b3 = tf.Variable(tf.constant(0.1, shape=[num_units2])) \n",
    "        hidden3 = tf.nn.relu(tf.matmul(h_pool3_flat, w3) + b3)\n",
    "        \n",
    "        hidden3_drop = tf.nn.dropout(hidden3, keep_prob)\n",
    "        \n",
    "        w0 = tf.Variable(tf.zeros([num_units2, 5]))\n",
    "        b0 = tf.Variable(tf.zeros([5]))\n",
    "        \n",
    "        tf.summary.histogram('weights_output', w0)\n",
    "        tf.summary.histogram('biases_output', b0)\n",
    "        \n",
    "    with tf.name_scope('sotmax'):\n",
    "        p = tf.nn.softmax(tf.matmul(hidden3_drop, w0) + b0)\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss(p, labels):\n",
    "    loss = -tf.reduce_sum(labels * tf.log(tf.clip_by_value(p, 1e-10, 1.0)))\n",
    "    \n",
    "    tf.summary.scalar('loss', loss)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(p, labels):\n",
    "    correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(labels, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimizer(loss):\n",
    "    train_step = tf.train.AdamOptimizer(1e-5).minimize(loss)\n",
    "\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"shuffle_batch:0\", shape=(100, 56, 56, 3), dtype=float32)\n",
      "Step: 0, Loss: 172.75, Accuracy: 0.390000\n",
      "Step: 10, Loss: 127.35, Accuracy: 0.520000\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    images, labels = load(['train.csv'])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    p = inference(images, keep_prob=keep_prob)\n",
    "    loss = loss(p, labels)\n",
    "    train_step = optimizer(loss)\n",
    "    accuracy = accuracy(p, labels)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    tf.train.start_queue_runners(sess)\n",
    "    \n",
    "    summary = tf.summary.merge_all()\n",
    "    summary_writer = tf.summary.FileWriter(LOGDIR, sess.graph)\n",
    "        \n",
    "    start_time = time.time()\n",
    "    for step in range(11):\n",
    "        \n",
    "        _, loss_val, acc_val = sess.run([train_step, loss, accuracy], feed_dict={keep_prob: 0.99})\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            loss_val, acc_val = sess.run([loss, accuracy], feed_dict={keep_prob: 1.0})\n",
    "            print('Step: %d, Loss: %.2f, Accuracy: %f' % (step, loss_val, acc_val))\n",
    "            \n",
    "            summary_str = sess.run(summary, feed_dict={keep_prob: 1.0})\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            saver.save(sess, 'cnn-session', global_step=step)\n",
    "    \n",
    "    duration = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
